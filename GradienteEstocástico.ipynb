{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradienteEstocástico.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jabazer/tao-fing/blob/master/GradienteEstoc%C3%A1stico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxNP2uruteMQ"
      },
      "source": [
        "# Import packages.\n",
        "import cvxpy as cp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import pylab as pl\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyHK-cAGeFGw"
      },
      "source": [
        "En este último notebook volvemos al método más sencillo de optimización, descenso por gradiente, pero agregamos ruido modelado como variables aleatorias o procesos estocásticos.\n",
        "\n",
        "El resultante es un método para diseñar algoritmos que procesen datos aleatorios recursivamente, y es base de tres familias de algoritmos de aprendizaje automático\n",
        "\n",
        "1) Reinforcement learning\n",
        "\n",
        "2) Neural networks\n",
        "\n",
        "3) Linear mean square filters\n",
        "\n",
        "En este caso tenemos que la función a minimizar es la esperanza de una función aleatoria\n",
        "\n",
        "$$f(\\theta)=E_{\\xi}\\left[F(\\theta,\\xi)\\right]$$\n",
        "\n",
        "\n",
        "$$F(\\theta)=f(\\theta)+\\eta$$\n",
        "\n",
        "donde $\\xi$ es la variable aleatoria, y $\\theta$ la variable determinística que \n",
        "queremos obtener.\n",
        "\n",
        "Luego queremos resolver \n",
        "\n",
        "\\begin{align}\n",
        "\\min_{\\theta\\in \\mathcal \\Theta}& f(\\theta)\\\\\n",
        "=\\min_{\\theta\\in  \\Theta}& E_{\\xi}\\left[F(\\theta,\\xi)\\right]\\\\\n",
        "=\\min_{\\theta\\in  \\Theta}& \\int p(\\xi)F(\\theta,\\xi) d\\xi\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Si aplicamos descenso por gradiente a este problema tenemos \n",
        "\n",
        "\\begin{align}\n",
        "\\theta^{k+1}&=\\theta^k+\\alpha_k \\nabla_\\theta f(\\theta^k)\\\\\n",
        "\\theta^{k+1}&=\\theta^k+\\alpha_k \\nabla_\\theta E_{\\xi}\\left[F(\\theta,\\xi)\\right]\n",
        "\\end{align}\n",
        "\n",
        "Si pudieramos calcular la esperanza, entonces simplemente corremos el algoritmo de arriba. \n",
        "\n",
        "Si en cambio tenemos una muestra de la variable aleatoria $\\xi^k$, o mejor el del gradiente estocástico entonces podemos simplemente tirar la esperanza y usar una muestra\n",
        "\n",
        "\\begin{align}\n",
        "\\hat \\theta^{k+1}&=\\hat \\theta^k+\\alpha_k \\nabla_\\theta  F(\\theta^k,\\xi^k)\n",
        "\\end{align}\n",
        "\n",
        "Idea, el gradiente es un acumulador que acumula promedios recursivamente, y estos aproximan la esperanza \n",
        "\n",
        "De hecho uno no tiene por qué quedarse con una sola muestra y puede hacer bacth stochastic gradient descent\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\hat \\theta^{k+1}&=\\hat \\theta^k+\\alpha_k \\sum_{n=1}^N \\nabla_\\theta  F(\\theta^k,\\xi^k_n)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Es algoritmo de SGD corresponde a usar una sola muestra por iteración  $N=1$\n",
        "\n",
        "Ejemplo\n",
        "\n",
        "$$y=A\\theta_0+\\xi, \\text{ con } \\xi_i\\sim \\mathcal N(0,\\sigma^2),\\ i=1,2$$\n",
        "\n",
        "Queremos resolver\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{\\theta\\in \\mathbb R^2}E\\left[||A\\theta -y||^2\\right]\n",
        "\\end{align}\n",
        "\n",
        "El gradiente es \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "E\\left[2A^TA\\theta -2A^Ty\\right]\n",
        "\\end{align}\n",
        "\n",
        "El gradiente estocástico\n",
        "\n",
        "\\begin{align}\n",
        "2A^TA\\theta -2A^Ty\n",
        "\\end{align}\n",
        "\n",
        "para ello necesitamos muestras de la variable aleatoria $y$\n",
        "\n",
        "Entonces corremos\n",
        "\\begin{align}\n",
        "\\hat \\theta^{k+1}&=\\hat \\theta^k+\\alpha_k\\left(2A^TA\\theta^k -2A^Ty^k\\right) \n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXMhF2-onCjr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "7ef188ea-e546-49d7-dc3d-f2d89d6e7667"
      },
      "source": [
        "sigma=2\n",
        "#sigma=0\n",
        "A=[[2, 1],\n",
        "   [1, 2]]\n",
        "I=[[1, 0],\n",
        "   [0, 1]]\n",
        "AT=np.transpose(A)\n",
        "ATA=np.dot(AT,A)\n",
        "theta_0=np.transpose([1,1])\n",
        "trayectoria0=np.array([])\n",
        "trayectoria1=np.array([])\n",
        "theta=np.array([0,0])\n",
        "for k in np.arange(500):\n",
        "    trayectoria0=np.append(trayectoria0,theta[0])\n",
        "    trayectoria1=np.append(trayectoria1,theta[1])\n",
        "    alpha=1/(10*k+1000)\n",
        "    alpha=1/(200)\n",
        "    xi = np.random.normal(0, sigma, 2)\n",
        "    y=np.dot(A,theta_0)+xi;\n",
        "    gradiente_estocastico=2*np.dot(ATA,theta)-2*np.dot(AT,y)\n",
        "    theta=theta-alpha*gradiente_estocastico;\n",
        "pl.plot(trayectoria0,trayectoria1,'bo-')\n",
        "pl.xlim(0, 2)\n",
        "pl.ylim(0, 2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 2.0)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl4jWigodFtV"
      },
      "source": [
        "#¿Cómo usar SGD para procesar datos recursivamente?\n",
        "\n",
        "##Preliminares sobre clasificación\n",
        "Consideremos el caso en que queremos clasificar muestras linealmente.\n",
        "Tenemos vectores $x_i\\in\\mathbb R^N$ y queremos clasificarlas en dos clases, asignandole una etiqueta $y_i\\in{0,1}$.\n",
        "\n",
        "El clasificador lineal que hace esto es $$y_i=(\\text{signo}(a^T x_i)+1)/2$$\n",
        "\n",
        "Esto significa que \n",
        "\n",
        "$$y_i=\\begin{cases}0 &\\text{ si  }\\  a^T x_i<0\\\\\n",
        "1 &\\text{ si  }\\  a^T x_i>0\n",
        "\\end{cases}$$\n",
        "\n",
        "y puede escribirse en dos pasos para simplificar notación\n",
        "\n",
        "\\begin{align}\n",
        "z_i&=\\alpha^T x_i\\\\\n",
        "y_i&=C(z_i)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "donde $$C(z):=(\\text{signo}(z)+1)/2=\\begin{cases}0 &\\text{ si  }\\  z<0\\\\\n",
        "1 &\\text{ si  }\\  z>0\n",
        "\\end{cases}$$\n",
        "\n",
        "\n",
        "Entonces el clasificador lineal con menos error cuadrático medio es el que minimiza\n",
        "\n",
        "$$\\min_{a\\in\\mathbb R^N}E_{x,y}\\left[\\left(y-C(a^Tx)\\right)^2\\right]$$\n",
        "\n",
        "\n",
        "Podríamos intentar SGD sobre este costo, pero primero vamos a introducir la rectified linear unit (RELU) como aproximación convexa del clasificador lineal \n",
        "substituyendo  $C(z)$ por \n",
        "\n",
        "\n",
        "$$\\text{RELU}(z):=\\begin{cases}0 &\\text{ si  }\\  z<0\\\\\n",
        "z &\\text{ si  }\\  z>0\n",
        "\\end{cases}$$\n",
        "\n",
        "\n",
        "Se vio que esta aproximación convexa era demasiado difícil de entrenar porque cualquier dato que cayera en la zona $\\alpha^T x$ no era sensible a una perturbación en $\\alpha$, entonces se adoptó\n",
        "\n",
        "\n",
        "$$\\text{RELU}_\\epsilon(z):=\\begin{cases}\\epsilon z &\\text{ si  }\\   z<0\\\\\n",
        "z &\\text{ si  }\\  z>0\n",
        "\\end{cases}$$\n",
        "\n",
        "Finalmente buscamos el clasificador óptimo que resuleva\n",
        "\n",
        "\n",
        "$$\\min_{a\\in\\mathbb R^N}E_{x,y}\\left[\\left(y-\\text{RELU}_\\epsilon(a^Tx)\\right)^2\\right]$$\n",
        "\n",
        "\n",
        "\n",
        "Por último, queremos clasificadores afines con $z=\\alpha^Tx+\\beta$, lo que se convierte en lineal si  adjuntamos un uno al final de cada vector $x_i$ y redefinimos \n",
        "\n",
        "$$ \\bar x_i\\leftarrow \\begin{pmatrix} x_i\\\\1\n",
        "\\end{pmatrix} \\quad y \\quad  a \\leftarrow \\begin{pmatrix} \\alpha\\\\\\beta\n",
        "\\end{pmatrix}$$ \n",
        "\n",
        "Con esto tenemos que $a^T\\bar x_i=\\alpha^T x_i+\\beta$ y podemos seguir con el caso lineal (sin escribir la barra).\n",
        "\n",
        "\n",
        "\n",
        "$$\\min_{a\\in\\mathbb R^{N+1}}E_{x,y}\\left[\\left(y-\\text{RELU}_\\epsilon(a^Tx)\\right)^2\\right]$$\n",
        "\n",
        "\n",
        "Teniendo que resolver este problema podemos usar SGD\n",
        "\n",
        "$$a^{k+1}=a^k+\\eta_k \\left(y^k-\\text{RELU}_\\epsilon(a^Tx^k)\\right) \\nabla_a \\text{RELU}_\\epsilon(a^Tx^k)$$ \n",
        "\n",
        "\n",
        "\n",
        "#Sobre la convergencia de procesos estocásticos\n",
        "\n",
        "La convergencia en error cuadrático medio (m.s.) no implica la convergencia con probabilidad 1 (a.s.). Abajo hay un ejemplo.\n",
        "\n",
        "El teorema de Robbins Monró implica la convergencia en m.s. \n",
        "Con marttingalas y usando el teorema de Doob puede probarse convergencia de SGD a.s.\n",
        "\n",
        "Ejemplo: Sea $X_{1}, X_{2}, \\ldots, X_{n}, \\ldots$ una sucesión de variables aleatorias tal que\n",
        "$$\n",
        "X_{n}= \\begin{cases}0 & \\text { with probability } 1-\\frac{1}{n} \\\\ 1 & \\text { with probability } \\frac{1}{n}\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "Converge a  $0$ en m.s., Pero no converge a.s. }\n",
        "\n",
        "\n",
        "Prueba: Sea $0<\\epsilon<1$, y para cualquier  $m$\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathrm{P}\\left\\{\\left|X_{n}-0\\right|<\\epsilon \\text { for all } n \\geq m\\right\\} &=\\lim _{n \\rightarrow \\infty} \\prod_{i=m}^{n}\\left(1-\\frac{1}{i}\\right) \\\\\n",
        "&=\\lim _{n \\rightarrow \\infty} \\prod_{i=m}^{n}\\left(\\frac{i-1}{i}\\right) \\\\\n",
        "&=\\lim _{n \\rightarrow \\infty} \\frac{(m-1)}{m} \\frac{m}{(m+1)} \\cdots \\frac{(n-1)}{n} \\\\\n",
        "&=\\lim _{n \\rightarrow \\infty} \\frac{m-1}{n} \\rightarrow 0 \\neq 1\n",
        "\\end{aligned}\n",
        "$$"
      ]
    }
  ]
}